{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "82e6a33e-517f-44a9-922d-d527e93bf268",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pysam\n",
    "import re\n",
    "import csv\n",
    "from collections import defaultdict\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "10600488-5d54-4160-9c13-5e22691b37b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_ids = [\n",
    "    \"ERR3579819\", \"ERR3579736\",\"ERR3579812\", \"ERR3579813\", \"ERR3579814\", \"ERR3579815\", \"ERR3579816\"\n",
    "]\n",
    "\n",
    "sample_map = {\n",
    "    \"ERR3579819\": \"VLC001\",\n",
    "    \"ERR3579736\": \"EMN001\",\n",
    "    \"ERR3579812\": \"TAF017\",\n",
    "    \"ERR3579813\": \"TAF017\",\n",
    "    \"ERR3579814\": \"TAF017\",\n",
    "    \"ERR3579815\": \"TAF017\",\n",
    "    \"ERR3579816\": \"TAF017\"\n",
    "}\n",
    "\n",
    "# Create a regex pattern for the sample substrings\n",
    "sample_pattern = re.compile(f\"({'|'.join(sample_ids)})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "124952db-d741-4c54-b288-15f71ceda386",
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_reads_per_reference(bam_file, output_tsv):\n",
    "    # Open BAM file\n",
    "    bam = pysam.AlignmentFile(bam_file, \"rb\")\n",
    "    \n",
    "    # Create a set of unique sample names\n",
    "    sample_names = set(sample_map.values())\n",
    "    \n",
    "    # Dictionary to hold counts per reference\n",
    "    counts_per_reference = defaultdict(lambda: {sample_name: 0 for sample_name in sample_names})\n",
    "    \n",
    "    # Iterate through each read in the BAM file\n",
    "    for read in bam.fetch(until_eof=True):\n",
    "        # Get the reference name (contig) the read is aligned to\n",
    "        reference_name = bam.get_reference_name(read.reference_id)\n",
    "        \n",
    "        # Extract the sample identifier from the read name\n",
    "        match = sample_pattern.search(read.query_name)\n",
    "        if match:\n",
    "            sample_id = match.group(0)\n",
    "            # Map the sample ID to the sample name\n",
    "            sample_name = sample_map.get(sample_id)\n",
    "            if sample_name:\n",
    "                # Increment the counter for the appropriate sample and reference\n",
    "                counts_per_reference[reference_name][sample_name] += 1\n",
    "    \n",
    "    # Convert sample_names to a sorted list for consistent column ordering\n",
    "    sample_names = sorted(sample_names)\n",
    "    \n",
    "    # Write results to a TSV file\n",
    "    with open(output_tsv, \"w\", newline=\"\") as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        # Write header\n",
    "        writer.writerow([\"Reference\"] + sample_names)\n",
    "        \n",
    "        # Write counts for each reference\n",
    "        for reference, sample_counts in counts_per_reference.items():\n",
    "            writer.writerow([reference] + [sample_counts[sample] for sample in sample_names])\n",
    "    \n",
    "    print(f\"Counts have been written to {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "caf92798-72d7-4c8e-8030-9316b2618abc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_counts_tsv(counts_tsv):\n",
    "    counts_dict = {}\n",
    "    with open(counts_tsv, \"r\") as file:\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        # Get the sample names from the header (excluding 'Reference')\n",
    "        sample_names = [field for field in reader.fieldnames if field != 'Reference']\n",
    "        for row in reader:\n",
    "            reference = row['Reference']\n",
    "            counts_dict[reference] = {sample: int(row[sample]) for sample in sample_names}\n",
    "    return counts_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7d99198e-0a97-4c32-b302-85aca1230d78",
   "metadata": {},
   "outputs": [],
   "source": [
    "def merge_tsvs(counts_tsv, cluster_tsv, output_tsv):\n",
    "    # Load counts data\n",
    "    counts_dict = load_counts_tsv(counts_tsv)\n",
    "    \n",
    "    with open(cluster_tsv, \"r\") as infile, open(output_tsv, \"w\", newline='') as outfile:\n",
    "        reader = csv.DictReader(infile, delimiter='\\t')\n",
    "        # Get sample names from counts_tsv\n",
    "        with open(counts_tsv, \"r\") as counts_file:\n",
    "            counts_reader = csv.DictReader(counts_file, delimiter='\\t')\n",
    "            sample_names = [field for field in counts_reader.fieldnames if field != 'Reference']\n",
    "        \n",
    "        # Add the count headers to the output file\n",
    "        fieldnames = reader.fieldnames + sample_names\n",
    "        writer = csv.DictWriter(outfile, fieldnames=fieldnames, delimiter='\\t')\n",
    "        \n",
    "        # Write the header\n",
    "        writer.writeheader()\n",
    "        \n",
    "        # Process each row from the second TSV\n",
    "        for row in reader:\n",
    "            # Extract the reference name from the SampleName column\n",
    "            sample_name = row['SampleName']\n",
    "            # Assume reference is everything between 'md_' and '_n' in SampleName\n",
    "            reference_parts = sample_name.split('md_')\n",
    "            if len(reference_parts) > 1:\n",
    "                reference = reference_parts[1].split('_n')[0]\n",
    "            else:\n",
    "                reference = sample_name  # Fallback if pattern doesn't match\n",
    "            \n",
    "            # Check if the reference exists in the counts dictionary\n",
    "            if reference in counts_dict:\n",
    "                # Add the counts from the first TSV to the current row\n",
    "                row.update(counts_dict[reference])\n",
    "            else:\n",
    "                # If no counts are available, fill with 0\n",
    "                row.update({sample: 0 for sample in sample_names})\n",
    "            \n",
    "            # Write the updated row to the output file\n",
    "            writer.writerow(row)\n",
    "    \n",
    "    print(f\"Output written to {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29be3df7-4fda-4ec4-a397-0b8b256138ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_merged_tsv(merged_tsv, k_cluster):\n",
    "    data = []\n",
    "    with open(merged_tsv, \"r\") as file:\n",
    "        reader = csv.DictReader(file, delimiter='\\t')\n",
    "        # Get sample names dynamically\n",
    "        sample_names = [field for field in reader.fieldnames if field not in ['SampleID', 'SampleName'] and not field.startswith('Cluster')]\n",
    "        for row in reader:\n",
    "            # Find the cluster with the highest probability for this reference\n",
    "            cluster = max(range(1, k_cluster + 1), key=lambda i: float(row.get(f'Cluster{i}', 0)))\n",
    "            \n",
    "            # Store relevant fields: reference name, sample counts, and assigned cluster\n",
    "            sample_counts = {sample: int(row[sample]) for sample in sample_names}\n",
    "            data.append({\n",
    "                'SampleID': row['SampleID'],\n",
    "                'Reference': row['SampleName'],\n",
    "                'Cluster': cluster,  # The cluster with the highest probability\n",
    "                **sample_counts\n",
    "            })\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eba0ec4c-1fff-4534-9af3-d26d8c2fecb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_fractions_relative_to_total(merged_tsv, output_tsv, sample_map, k_cluster):\n",
    "    # Load merged data\n",
    "    data = load_merged_tsv(merged_tsv, k_cluster)\n",
    "    \n",
    "    # Get unique sample names from sample_map\n",
    "    sample_names = sorted(set(sample_map.values()))\n",
    "    \n",
    "    # Dictionary to store total reads and sample-specific reads per cluster\n",
    "    cluster_totals = defaultdict(lambda: {'total': 0, **{sample: 0 for sample in sample_names}})\n",
    "    \n",
    "    # Global totals to calculate the total aligned reads across all clusters\n",
    "    global_totals = {'total': 0, **{sample: 0 for sample in sample_names}}\n",
    "    \n",
    "    # Aggregate read counts by the assigned cluster and globally\n",
    "    for row in data:\n",
    "        cluster = row['Cluster']\n",
    "        # Sum counts for all samples in this row\n",
    "        total_reads = sum(row[sample] for sample in sample_names)\n",
    "        \n",
    "        # Update total reads and sample-specific reads for the assigned cluster\n",
    "        cluster_totals[cluster]['total'] += total_reads\n",
    "        for sample in sample_names:\n",
    "            cluster_totals[cluster][sample] += row[sample]\n",
    "        \n",
    "        # Update global totals\n",
    "        global_totals['total'] += total_reads\n",
    "        for sample in sample_names:\n",
    "            global_totals[sample] += row[sample]\n",
    "    \n",
    "    # Variables to store the sum of all fractions per sample\n",
    "    total_fractions = {sample: 0 for sample in sample_names}\n",
    "    \n",
    "    # Write the results to a new TSV file\n",
    "    with open(output_tsv, \"w\", newline=\"\") as tsvfile:\n",
    "        writer = csv.writer(tsvfile, delimiter='\\t')\n",
    "        \n",
    "        # Write the header\n",
    "        header = ['Cluster']\n",
    "        # Add fraction of total reads columns\n",
    "        header += [f\"{sample}_Fraction_of_Total\" for sample in sample_names]\n",
    "        # Add fraction within cluster columns\n",
    "        header += [f\"{sample}_Fraction_of_Cluster\" for sample in sample_names]\n",
    "        writer.writerow(header)\n",
    "        \n",
    "        # Calculate and write the fractions for each cluster compared to total reads\n",
    "        for cluster in sorted(cluster_totals.keys()):\n",
    "            counts = cluster_totals[cluster]\n",
    "            row = [cluster]\n",
    "            \n",
    "            # Calculate fractions of total mapped reads\n",
    "            fractions_of_total = []\n",
    "            for sample in sample_names:\n",
    "                if global_totals['total'] > 0:\n",
    "                    fraction = round(counts[sample] / global_totals['total'], 4)\n",
    "                else:\n",
    "                    fraction = 0\n",
    "                fractions_of_total.append(fraction)\n",
    "                total_fractions[sample] += fraction  # Sum up for the final row\n",
    "            \n",
    "            # Calculate fractions within each cluster\n",
    "            fractions_within_cluster = []\n",
    "            total_reads_cluster = counts['total']\n",
    "            for sample in sample_names:\n",
    "                if total_reads_cluster > 0:\n",
    "                    fraction = round(counts[sample] / total_reads_cluster, 4)\n",
    "                else:\n",
    "                    fraction = 0\n",
    "                fractions_within_cluster.append(fraction)\n",
    "            \n",
    "            # Combine all fractions into the row\n",
    "            row += fractions_of_total + fractions_within_cluster\n",
    "            writer.writerow(row)\n",
    "        \n",
    "        # Add the final row with sums of the fractions of total reads\n",
    "        total_row = ['Total']\n",
    "        total_row += [round(total_fractions[sample], 4) for sample in sample_names]\n",
    "        # Add empty cells for fractions within cluster\n",
    "        total_row += ['' for _ in sample_names]\n",
    "        writer.writerow(total_row)\n",
    "    \n",
    "    print(f\"Cluster-specific fractions relative to total reads with sums have been written to {output_tsv}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b1f444b-09e9-4e37-88e8-ddfed0d580bb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2f95d73a-f09e-42bb-af17-e4d2ff46eedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = 3\n",
    "out = \"VLCEMNTAF/min10k\"\n",
    "os.makedirs(out, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "b5e6224d-59e6-45da-93eb-41bca98858fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Specify the input BAM file and output TSV file\n",
    "#bam_file = \"/home/project2/data/empirical/yates2021/results/alignments/VLC_EMN_PYK_sorted_md.bam\"\n",
    "bam_file = \"/home/project2/data/empirical/yates2021/results/alignments/VLC_EMN_TAF_min10k_sorted_md.bam\"\n",
    "cluster_tsv = f\"/home/project2/bam2profDevs/bam2prof_production/yates_vlc_emn_taf_min10k_v2_plots/GMM/k{cluster}/cluster_report_k{cluster}.tsv\"\n",
    "reads_per_ref = f\"{out}/output_counts.tsv\"\n",
    "merge_tsv = f\"{out}/merged_output_k{cluster}.tsv\"\n",
    "fractions = f\"{out}/cluster_fractions_k{cluster}.tsv\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0660dc13-3d0b-4762-b529-1195cfa1b651",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts have been written to VLCEMNTAF/min10k/output_counts.tsv\n"
     ]
    }
   ],
   "source": [
    "# Run the function\n",
    "count_reads_per_reference(bam_file, reads_per_ref)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "4803c0a3-ac8b-4d14-8b3b-3ff83bc0915c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Output written to VLCEMNTAF/min10k/merged_output_k3.tsv\n"
     ]
    }
   ],
   "source": [
    "merge_tsvs(reads_per_ref, cluster_tsv, merge_tsv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "62600390-0dbd-465f-a118-2313d9dfc13c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster-specific fractions relative to total reads with sums have been written to VLCEMNTAF/min10k/cluster_fractions_k3.tsv\n"
     ]
    }
   ],
   "source": [
    "calculate_fractions_relative_to_total(merge_tsv, fractions, sample_map, cluster)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
